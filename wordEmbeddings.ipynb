{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Why we need to convert words into vector?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're converting words to numerical value (in the form of vectors) so that it can be passed to a neural network for training. Vectors also helps us to understand text similarity by measuring cosine between two vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** What are the differences in between CBOW (Continuous Bag-of-Words) and Skip-gram?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both are architectures to learn the underlying word representations for each word by using neural networks. In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle. While in the Skip-gram model, the distributed representation of the input word is used to predict the context.\n",
    "\n",
    "Skip Gram works well with small amount of data and is found to represent rare words well.\n",
    "On the other hand, CBOW is faster and has better representations for more frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Is CBOW and BOW (Bag-of-words) are the same?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words(BOW) is an approach for dealing with words and context in text processing or information retrieval. It refers to a way in which a group of words are represented without retaining order.Consider the below sentence.\n",
    "“I had a great time today.”\n",
    "The BOW representations of the sentence would be the collection of the six words without order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW is always with respect to a word in the sequence. In CBOW you consider other words in the line based on their position w.r.t to the focus word.\n",
    "\n",
    " Consider the word ‘great’ as the word of focus from the above line.Words before i.e “I”, “had” and “a” are considered history words. Words after i.e “time” and “today” are considered future words.\n",
    " So history Bag of Words for ‘great’ is (“I”, “had”, “a”) without order and future Bag of Words for is (“time”, “today”) also without order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** What are the use cases of CBOW and Skip-gram?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " CBOW is learning to predict the word by the context. Or maximize the probability of the target word by looking at the context. And this happens to be a problem for rare words. For example, given the context yesterday was a really [...] day CBOW model will tell you that most probably the word is beautiful or nice. Words like delightful will get much less attention of the model, because it is designed to predict the most probable word. This word will be smoothed over a lot of examples with more frequent words.\n",
    "\n",
    "On the other hand, the skip-gram model is designed to predict the context. Given the word delightful it must understand it and tell us that there is a huge probability that the context is yesterday was really [...] day, or some other relevant context. With skip-gram the word delightful will not try to compete with the word beautiful but instead, delightful+context pairs will be treated as new observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.**  Do you know any alternatives of Word2Vec?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were many ways to leverage the next word prediction into different contextual learning tasks. \n",
    "\n",
    "Doc2Vec takes a whole sentence as input and generates representation at a sentence level. \n",
    "\n",
    "Lda2Vec’s work combines word representation and semantic representation learned from topics.\n",
    "\n",
    "GloVE model combines word embeddings with statistical information. Stanford’s GloVE is one of the most popular alternatives of Google’s Word2Vec model. \n",
    "\n",
    "Facebook’s FastText model uses character n-grams and an efficient learning process to learn embeddings for out of the vocabulary words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** Define Word Embedding in your own words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding means representing each word by a vector. It is capable of capturing the semantic and syntactic similarity. Google's word2vec is one of the most popular ways to generate word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
