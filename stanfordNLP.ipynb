{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** What are the differences in between NLTK and Stanfordnlp? What things we can easily do in NLTK but not in Stanfordnlp and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main functional difference is that NLTK has multiple versions or interfaces to other versions of NLP tools, while Stanford CoreNLP only has their version. NLTK also supports installing third-party Java projects, and even includes instructions for installing some Stanford NLP packages on the wiki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one fundamental difference is, you can't parse syntactic dependencies out of the box with NLTK. You need to specify a Grammar for that which can be very tedious if the text domain is not restricted. Whereas Standford NLP provides a probabilistic parser for general text as a down-loadable model, which is quite accurate. It also has built in NER (Named Entity Recognition) and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Which NLP library is best for dependency parsing?And Explain,Why that library is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency parsing is the process of analyzing the grammatical structure of a sentence based on the dependencies between the words in a sentence.\n",
    "SpaCy is the best when it comes to dependency parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As spaCy uses the latest and best algorithms, its performance is usually good as compared to NLTK. In word tokenization and POS-tagging spaCy performs better, but in sentence tokenization, NLTK outperforms spaCy. Its poor performance in sentence tokenization is a result of differing approaches: NLTK attempts to split the text into sentences. In contrast, spaCy constructs a syntactic tree for each sentence, a more robust method that yields much more information about the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Is Documents and sentence are same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentence is a sequence of words whereas a document is a sequence of sentences. Both can be of any length.\n",
    "If you wanted, you could treat an individual paragraph or even sentence as a \"document\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** What is the use of Multi-word tokens property in Stanfordnlp?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multi-Word Token (MWT) expansion module can expand a raw token into multiple syntactic words, which makes it easier to carry out Universal Dependencies analysis in some languages. This was handled by the MWTProcessor in Stanza, and can be invoked with the name mwt. The token upon which an expansion will be performed is predicted by the TokenizeProcessor, before the invocation of the MWTProcessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
